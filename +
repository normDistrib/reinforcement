import numpy as np
import matplotlib.pyplot as plt


def greedy(n,ts,tasks,e=0.01):
    """
    n-Armed Bandit with value estimation by averaging previous rewards.
    If greedy = True performs greedy approach searching for maximum reward
    at each step.
    If greedy = False permorms e-greedy approach.

    Params:
        - n: number of actions
        - ts: number of timesteps.
        - tasks: number of n-Armed Bandit tasks generated.
        - greedy: if greedy = True performs greedy approach searching for 
                  maximum reward at each step. If greedy = False permorms 
                  e-greedy approach.
        - e: probability of choosing randomly between actions:
    Select always the best one
    """

    total_rewards = np.zeros(ts)

    for _ in range(tasks):
        vals = np.random.randn(n) # real values
        N = np.ones(n) # number of time selected an action
        estimates = np.zeros(n) # estimations of values for each action
        rewards = [] # rewards obtained at each step

        pos = np.random.randint(0,n)
        R = vals[pos] + np.random.randn(1)[0] # R_k = q(A) + random_noice
        estimates[pos] = estimates[pos]+1/N[pos]*(R-estimates[pos])
        rewards.append(R)
        N[pos] += 1

        for _ in range(1,ts):
            pos = np.random.choice([np.random.randint(0,n),np.argmax(estimates)],
                                   size = 1,
                                   p = [e,1-e])[0]
            N[pos] += 1
            R = vals[pos] + np.random.randn(1)[0]
            estimates[pos] = estimates[pos]+1/N[pos]*(R-estimates[pos])
            rewards.append(R)

        total_rewards += np.array(rewards)

    return total_rewards/tasks

def _e_greedy(n,ts,tasks,e):
    """
    Select almost always the best one, but sometimes
    (with probability e) select action with same
    probability for each.

    Tried with different implementation, but obtained same long time result.
    """

    total_rewards = np.zeros(ts)

    for _ in range(tasks):
        vals = np.random.randn(n) # real values
        N = np.ones(n) # number of time selected an action
        estimates = np.zeros(n) # estimations of values for each action
        rewards = [] # rewards obtained at each step

        pos = np.random.randint(0,n)
        R = vals[pos] + np.random.randn(1)[0] # R_k = q(A) + random_noice
        estimates[pos] = estimates[pos]+1/N[pos]*(R-estimates[pos])
        rewards.append(R)
        N[pos] += 1

        for _ in range(1,ts):
            pos = np.random.choice([np.random.randint(0,n),np.argmax(estimates)],
                                   size = 1,
                                   p = [e,1-e])[0]
            N[pos] += 1
            R = vals[pos] + np.random.randn(1)[0]
            estimates[pos] = estimates[pos]+1/N[pos]*(R-estimates[pos])
            rewards.append(R)

        total_rewards += np.array(rewards)

    return total_rewards/tasks


if __name__ == '__main__':
    plt.plot(greedy(10,1000,100,0)) # greedy method
    plt.plot(greedy(10,1000,100,e=0.01))
    plt.plot(greedy(10,1000,100,greedy=False,e=0.1))
    plt.show()
